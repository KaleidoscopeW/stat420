---
title: 'STAT 420: Homework 05'
author: "Spring 2020, Yu Wu (yuw5)"
date: 'Due: Tuesday, March 3 by 11:30 PM CT'
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

# Assignment

## Exercise 1 (Using `lm`)

For this exercise we will use the data stored in [`nutrition.csv`](nutrition.csv). It contains the nutritional values per serving size for a large variety of foods as calculated by the USDA. It is a cleaned version totaling 5,138 observations and is current as of September 2015.

The variables in the dataset are:

- `ID` 
- `Desc` - Short description of food
- `Water` - in grams
- `Calories` 
- `Protein` - in grams
- `Fat` - in grams
- `Carbs` - Carbohydrates, in grams
- `Fiber` - in grams
- `Sugar` - in grams
- `Calcium` - in milligrams
- `Potassium` - in milligrams
- `Sodium` - in milligrams
- `VitaminC` - Vitamin C, in milligrams
- `Chol` - Cholesterol, in milligrams
- `Portion` - Description of standard serving size used in analysis

**(a)** Fit the following multiple linear regression model in `R`. Use `Calories` as the response and `Carbs`, `Fat`, and `Protein` as predictors.

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i.
\]

Here,

- $Y_i$ is `Calories`.
- $x_{i1}$ is `Carbs`.
- $x_{i2}$ is `Fat`.
- $x_{i3}$ is `Protein`.

Use an $F$-test to test the significance of the regression. Report the following:
 
- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
nutrition <- read.csv("nutrition.csv")

fit <- lm(Calories ~ Carbs + Fat + Protein, data = nutrition)
summary(fit)
```

- The null hypotheses is that all of the predictors are insignificant. The alternative hypothesis is that at least one of the predictors is significant. 
- The value of the test statistic is 1.524e05.
- The p-value of the test is 2.2e-16. 
- A statistical decision at $\alpha = 0.01$ is reject the null hypothesis since the p-value is smaller than alpha.
- A conclusion in the context of the problem is that the amount of calories of a food is related with at least one of the following predictors: Carbs, Fat and Protein.



**(b)** Output only the estimated regression coefficients. Interpret all $\hat{\beta}_j$ coefficients in the context of the problem.

```{r}
summary(fit)$coef[,1]
```

$$
Calories = 3.768 + 3.774*Carbs + 8.804*Fat + 3.967*Protein
$$

When there is no carbs, fat or protein, the amount of calories is 3.768; when carbs increases by one, the amount of calories increases by 3.774; when fat increases by one, the amount of calories increases by 8.804; when protein increases by one, the amount of calories increases by 3.967;

**(c)** Use your model to predict the amount of `Calories` in a Big Mac. According to [McDonald's publicized nutrition facts](http://nutrition.mcdonalds.com/getnutrition/nutritionfacts.pdf), the Big Mac contains 47g of carbohydrates, 28g of fat, and 25g of protein.

```{R}
3.768 + 3.774*47 + 8.804*28 + 3.967*25
```

$$
Calories_{BIG MAC}=3.768 + 3.774*47 + 8.804*28 + 3.967*25 = 526.833
$$

Big Mac has 526.833g calories.

**(d)** Calculate the standard deviation, $s_y$, for the observed values in the Calories variable. Report the value of $s_e$ from your multiple regression model. Interpret both estimates in the context of this problem.

```{r}
s_y <- sd(nutrition$Calories)
s_e <- summary(fit)$sigma

s_y
s_e
```

The standard deviation tells us how much the calories spread out within the dataset. The standard error tells us how much our residuals or the fitted values spread out within the fitted model. 

**(e)** Report the value of $R^2$ for the model. Interpret its meaning in the context of the problem.

```{r}
summary(fit)$r.squared
```

The $R^2$ of the model is 0.989. It means 98.9% of the data is explained by the fitted model. 

**(f)** Calculate a 90% confidence interval for $\beta_2$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(fit, level = 0.90)[3,]
```

We are 90% confident that the value of $\beta_2$ is between 8.779 and 8.829. 

**(g)** Calculate a 95% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(fit, level = 0.95)[1,]
```

We are 95% confident that the value of $\beta_0$ is between 2.803 and 4.733. 


**(h)** Use a 99% confidence interval to estimate the mean Calorie content of a small order of McDonald's french fries that has 30g of carbohydrates, 11g of fat, and 2g of protein. Interpret the interval in context.

```{r} 
x <- data.frame(Carbs = 30, Fat = 11, Protein = 2)
predict(fit, newdata = x, interval = 'confidence', level = 0.99)
```

We are 99% sure that the amount of calories of french fries with the given data is between 220.892g and 222.620g.

**(i)** Use a 90% prediction interval to predict the Calorie content of new healthy menu item that has 11g of carbohydrates, 1.5g of fat, and 1g of protein. Interpret the interval in context.

```{r} 
x <- data.frame(Carbs = 11, Fat = 1.5, Protein = 1)
predict(fit, newdata = x, interval = 'confidence', level = 0.90)
```

We are 90% sure that the amount of calories of french fries with the given data is between 61.77276g and 63.12954g.


## Exercise 2 (More `lm`)

For this exercise we will again use the nutrition data. 

**(a)** Fit a model with Calories as the response and `Carbs`, `Sodium`, `Fat`, and `Protein` as predictors. Use an $F$-test to test the significance of the regression. Report the following:
 
- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

```{r}
fit <- lm(Calories ~ Carbs + Sodium + Fat + Protein, data = nutrition)
summary(fit)
```

- The null hypotheses is all of the predictors are
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem


**(b)** For each of the predictors in part **(a)**, perform a $t$-test for the significance of its regression coefficient. Report the following for each:
 
- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$

```{r}
summary(fit)$coef
```

Carbs:   

- $H_0: \beta_{Carbs}=0$ VS $H_a: \beta_{Carbs} \neq0$
- $t=388.717271$
- $p-value = 0$
- A statistical decision at $\alpha = 0.01$ is reject the null.

Sodium:   

- $H_0: \beta_{Sodium}=0$ VS $H_a: \beta_{Sodium} \neq0$
- $t=1.362579$
- $p-value = 1.730749e-01$
- A statistical decision at $\alpha = 0.01$ is fail to reject the null.

Fat:   

- $H_0: \beta_{Fat}=0$ VS $H_a: \beta_{Fat} \neq0$
- $t=575.261702$
- $p-value = 0$
- A statistical decision at $\alpha = 0.01$ is reject the null.

Protein:   

- $H_0: \beta_{Protein}=0$ VS $H_a: \beta_{Protein} \neq0$
- $t=150.533385$
- $p-value = 0$
- A statistical decision at $\alpha = 0.01$ is reject the null.



**(c)** Based on your results in part **(b)**, do you still prefer the model in part
**(a)**, or is there instead a model with three predictors that you prefer? Briefly explain.

No, because the Sodium predictor is insignificant, and we can drop it from the model. The reduced model would be the one in part 1(a).

## Exercise 3 (Comparing Models)

For this exercise we will use the data stored in [`goalies_cleaned.csv`](goalies_cleaned.csv). It contains career data for 462 players in the National Hockey League who played goaltender at some point up to and including the 2014 - 2015 season. The variables in the dataset are:
 
- `W` - Wins
- `GA` - Goals Against
- `SA` - Shots Against
- `SV` - Saves
- `SV_PCT` - Save Percentage
- `GAA` - Goals Against Average
- `SO` - Shutouts
- `MIN` - Minutes
- `PIM` - Penalties in Minutes
 
**(a)** Fit a multiple linear regression model with Wins as the response and all other variables as the predictors.
 
Use an $F$-test to test the significance of the regression. Report the following:
 
- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.10$
- A conclusion in the context of the problem
 
When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
goalies_cleaned <- read.csv("goalies_cleaned.csv")

fit <- lm(W ~ ., data = goalies_cleaned)
summary(fit)
```

- $H_0: \beta_1=\beta_2=\beta_3=\beta_4=\beta_5=\beta_6=\beta_7=\beta_8 =0$ VS $H_a: \beta_1=\beta_2=\beta_3=\beta_4=\beta_5=\beta_6=\beta_7=\beta_8 \neq0$
- $F=3938$
- $p-value = 2.2e-16$
- A statistical decision at $\alpha = 0.10$ is reject the null.
- Based on the F-Test, we can conclude that the number of wins is related with at least one of those predictors. 

**(b)** Calculate the RMSE of this full model. Report the residual standard error of this full model. What is the relationship of these two values?

Recall, we have defined RMSE as,

\[
RMSE = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}.
\]

```{r}
rmse <- sqrt(mean(resid(fit)^2))
rmse

summary(fit)$sigma
```

The two values are both estimators of standard deviation of errors. 

**(c)** Fit a model with Wins as the response and with Goals Against, Goals Against Average, Saves, and Save Percentage as the predictors. Calculate the RMSE of this model.

```{r}
fit2 <- lm(W ~ GA + GAA + SV + SV_PCT, data = goalies_cleaned)
summary(fit2)

rmse2 <- sqrt(mean(resid(fit2)^2))
rmse2
```

**(d)** Fit a model with Wins as the response and with Goals Against Average and Save Percentage as the predictors. Calculate the RMSE of this model.

```{r}
fit3 <- lm(W ~ GAA +SV_PCT, data = goalies_cleaned)
summary(fit3)

rmse3 <- sqrt(mean(resid(fit3)^2))
rmse3
```

**(e)** Based on the previous three models, which model is most helpful for predicting wins? Briefly explain.

The full model is most helpful amoung all the others, because it has the lowest RMSE. 


**(f)** Conduct an ANOVA $F$-test comparing the models in parts **(c)** and **(d)**. Report the following:

 
- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.10$
- A conclusion in the context of the problem
 
When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

 
```{r}
anova(fit3, fit2)
```

- $H_0: \beta_{GA}=\beta_{SV}=0$ VS $H_a: \beta_{GA}=\beta_{SV} \neq0$
- $F=3599.8 $
- $p-value = 2.2e-16$
- A statistical decision at $\alpha = 0.10$ is reject the null.
- Based on the F-Test, we can conclude that the model that contains GA and SV is better.


## Exercise 4 (Regression without `lm`)

For this exercise use the `prostate` dataset from the `faraway` package. Use `?prosate` to learn about the dataset. The goal of this exercise is to fit a model with `lpsa` as the response and the remaining variables as predictors.

**(a)** Obtain the estimated regression coefficients **without** the use of `lm()` or any other built-in functions for regression. That is, you should use only matrix operations. Store the results in a vector `beta_hat_no_lm`. To ensure this is a vector, you may need to use `as.vector()`. Return this vector as well as the results of `sum(beta_hat_no_lm)`.

```{r}
library("faraway")
data("prostate")

n = nrow(prostate)
x = as.matrix(cbind(rep(1, n), prostate[,1-ncol(prostate)-1]))
y = prostate$lpsa
beta_hat_no_lm = as.vector(solve(t(x) %*% x) %*% t(x) %*% y)
beta_hat_no_lm
sum(beta_hat_no_lm)
```

**(b)** Obtain the estimated regression coefficients **with** the use of `lm()`. Store the results in a vector `beta_hat_lm`. To ensure this is a vector, you may need to use `as.vector()`. Return this vector as well as the results of `sum(beta_hat_lm)`.

```{r}
fit <- lm(lpsa ~ ., data = prostate)
beta_hat_lm <- as.vector(summary(fit)$coef[, 1])
beta_hat_lm
sum(beta_hat_lm)
```

**(c)** Use the `all.equal()` function to verify that the results are the same. You may need to remove the names of one of the vectors. The `as.vector()` function will do this as a side effect, or you can directly use `unname()`.

```{r}
all.equal(beta_hat_lm, beta_hat_no_lm)
```

**(d)** Calculate $s_e$ without the use of `lm()`. That is, continue with your results from **(a)** and perform additional matrix operations to obtain the result. Output this result. Also, verify that this result is the same as the result obtained from `lm()`.

```{r}
p <- length(coef(fit))
y_hat <- x %*% solve(t(x) %*% x) %*% t(x) %*% y
e <- y - y_hat
s_e <- sqrt(t(e) %*% e / (n - p))

s_e
all.equal(summary(fit)$sigma,as.vector(s_e))
```

**(e)** Calculate $R^2$ without the use of `lm()`. That is, continue with your results from **(a)** and **(d)** and perform additional operations to obtain the result. Output this result. Also, verify that this result is the same as the result obtained from `lm()`.

```{r}
SST <- sum((y - mean(y)) ^ 2)
SSF <- sum((y_hat - mean(y)) ^ 2)
R2 <- SSF / SST

R2
all.equal(R2,summary(fit)$r.squared)
```